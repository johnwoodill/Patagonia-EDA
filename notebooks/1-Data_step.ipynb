{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import os as os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFW_DIR = '/data2/GFW_point/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "gfw_vessel_dat = pd.read_csv('~/Data/GFW_public/fishing_vessels/fishing_vessels.csv')\n",
    "dat = pd.read_csv('/data2/GFW_point/2016-01-01/messages-2016-01-01-000000000000.csv')\n",
    "#gfw_identities = gfw_identities.rename(index=str, columns={\"ssvid\": \"mmsi\"})\n",
    "#gfw_identities['year'] = pd.DatetimeIndex(gfw_identities['timestamp']).year \n",
    "#gfw_identities['month'] = pd.DatetimeIndex(gfw_identities['timestamp']).month\n",
    "#gfw_identities['day'] = pd.DatetimeIndex(gfw_identities['timestamp']).day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_step(data): \n",
    "    # Patagonia shelf coordinates\n",
    "    #lower left lat: -58\n",
    "    #lower left lon: -77\n",
    "    #upper right lat: -23\n",
    "    #upper right lon: -22\n",
    "    \n",
    "    lon1 = -77\n",
    "    lon2 = -22\n",
    "    lat1 = -58\n",
    "    lat2 = -23\n",
    "    retdat = data[(data['lon'] >= lon1) & (data['lon'] <= lon2) & (data['lat'] >= lat1) & (data['lat'] <= lat2)]\n",
    "    \n",
    "    # Separate Year, month, day, hour, minute, second\n",
    "    retdat.loc[:, 'timestamp'] = pd.to_datetime(retdat['timestamp'], format=\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "    retdat.loc[:, 'year'] = pd.DatetimeIndex(retdat['timestamp']).year \n",
    "    retdat.loc[:, 'month'] = pd.DatetimeIndex(retdat['timestamp']).month\n",
    "    retdat.loc[:, 'day'] = pd.DatetimeIndex(retdat['timestamp']).day\n",
    "    retdat.loc[:, 'hour'] = pd.DatetimeIndex(retdat['timestamp']).hour\n",
    "    retdat.loc[:, 'minute'] = pd.DatetimeIndex(retdat['timestamp']).minute\n",
    "    retdat.loc[:, 'second'] = pd.DatetimeIndex(retdat['timestamp']).second\n",
    "    \n",
    "    # Merge GFW ID data\n",
    "    retdat = pd.merge(retdat, gfw_vessel_dat, how='left', on='mmsi')  \n",
    "    \n",
    "    retdat = retdat[['timestamp', 'year', 'month', 'day', 'hour', 'minute', 'second', 'mmsi', 'lat', 'lon', \\\n",
    "                    'segment_id', 'message_id', 'type', 'speed', 'course', 'heading', 'shipname', 'callsign', \\\n",
    "                     'destination', 'elevation_m', 'distance_from_shore_m', 'distance_from_port_m', 'nnet_score', \\\n",
    "                     'logistic_score', 'flag', 'geartype', 'length', 'tonnage', 'engine_power', 'active_2012', \\\n",
    "                     'active_2013', 'active_2014', 'active_2015', 'active_2016']]\n",
    "    \n",
    "    return retdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GFW_directories():\n",
    "    \n",
    "    dirs = os.listdir(GFW_DIR)\n",
    "    # Remove subfolders 'BK' and 'identities'\n",
    "    if 'BK' in dirs:\n",
    "        dirs.remove('BK')\n",
    "    \n",
    "    if 'identities' in dirs:\n",
    "        dirs.remove('identities')\n",
    "    \n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data_step() got an unexpected keyword argument 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-e5a087a72a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mndat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data_step() got an unexpected keyword argument 'filename'"
     ]
    }
   ],
   "source": [
    "ndat = data_step(data=dat, filename='test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data = dat\n",
    "    lon1 = -77\n",
    "    lon2 = -22\n",
    "    lat1 = -58\n",
    "    lat2 = -23\n",
    "    retdat = data[(data['lon'] >= lon1) & (data['lon'] <= lon2) & (data['lat'] >= lat1) & (data['lat'] <= lat2)]\n",
    "    \n",
    "    # Separate Year, month, day, hour, minute, second\n",
    "    retdat.loc[:, 'timestamp'] = pd.to_datetime(retdat['timestamp'], format=\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "    retdat.loc[:, 'year'] = pd.DatetimeIndex(retdat['timestamp']).year \n",
    "    retdat.loc[:, 'month'] = pd.DatetimeIndex(retdat['timestamp']).month\n",
    "    retdat.loc[:, 'day'] = pd.DatetimeIndex(retdat['timestamp']).day\n",
    "    retdat.loc[:, 'hour'] = pd.DatetimeIndex(retdat['timestamp']).hour\n",
    "    retdat.loc[:, 'minute'] = pd.DatetimeIndex(retdat['timestamp']).minute\n",
    "    retdat.loc[:, 'second'] = pd.DatetimeIndex(retdat['timestamp']).second\n",
    "    \n",
    "    # Merge GFW ID data\n",
    "    retdat = pd.merge(retdat, gfw_vessel_dat, how='left', on='mmsi')  \n",
    "    \n",
    "    retdat = retdat[['timestamp', \\\n",
    "                     'year', 'month', 'day', 'hour', 'minute', 'second', \\\n",
    "                     'mmsi', 'lat', 'lon', \\\n",
    "                    'segment_id', 'message_id', 'type', 'speed', 'course', 'heading', 'shipname', 'callsign', \\\n",
    "                     'destination', 'elevation_m', 'distance_from_shore_m', 'distance_from_port_m', 'nnet_score', \\\n",
    "                     'logistic_score', 'flag', 'geartype', 'length', 'tonnage', 'engine_power', 'active_2012', \\\n",
    "                     'active_2013', 'active_2014', 'active_2015', 'active_2016']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         3\n",
       "1         3\n",
       "2         3\n",
       "3         3\n",
       "4         3\n",
       "5         3\n",
       "6         3\n",
       "7         3\n",
       "8         3\n",
       "9         3\n",
       "10        3\n",
       "11        3\n",
       "12        3\n",
       "13        3\n",
       "14        3\n",
       "15        3\n",
       "16        3\n",
       "17        3\n",
       "18        3\n",
       "19        3\n",
       "20        3\n",
       "21        3\n",
       "22        3\n",
       "23        3\n",
       "24        3\n",
       "25        3\n",
       "26        3\n",
       "27        3\n",
       "28        3\n",
       "29        3\n",
       "         ..\n",
       "326713    3\n",
       "326714    3\n",
       "326715    3\n",
       "326716    3\n",
       "326717    3\n",
       "326718    3\n",
       "326719    3\n",
       "326720    3\n",
       "326721    3\n",
       "326722    3\n",
       "326723    3\n",
       "326724    3\n",
       "326725    3\n",
       "326726    3\n",
       "326727    3\n",
       "326728    3\n",
       "326729    3\n",
       "326730    3\n",
       "326731    3\n",
       "326732    3\n",
       "326733    3\n",
       "326734    3\n",
       "326735    3\n",
       "326736    3\n",
       "326737    3\n",
       "326738    3\n",
       "326739    3\n",
       "326740    3\n",
       "326741    3\n",
       "326742    3\n",
       "Name: day, Length: 326743, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retdat.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{ndat['year'][1]}-\" + f\"{ndat['month'][1]}\".zfill(2) + f\"-\" + f\"{ndat['day'][1]}\".zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndat.to_csv('~/Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2016-01-02 03:09:51+00:00\n",
       "1       2016-01-02 14:05:23+00:00\n",
       "2       2016-01-02 15:01:15+00:00\n",
       "3       2016-01-02 16:35:31+00:00\n",
       "4       2016-01-02 06:11:47+00:00\n",
       "5       2016-01-02 04:40:14+00:00\n",
       "6       2016-01-02 06:09:35+00:00\n",
       "7       2016-01-02 15:42:36+00:00\n",
       "8       2016-01-02 21:45:30+00:00\n",
       "9       2016-01-02 12:32:38+00:00\n",
       "10      2016-01-02 14:07:11+00:00\n",
       "11      2016-01-02 16:26:18+00:00\n",
       "12      2016-01-02 06:07:07+00:00\n",
       "13      2016-01-02 15:46:57+00:00\n",
       "14      2016-01-02 17:15:31+00:00\n",
       "15      2016-01-02 17:56:33+00:00\n",
       "16      2016-01-02 04:35:48+00:00\n",
       "17      2016-01-02 02:15:21+00:00\n",
       "18      2016-01-02 02:09:23+00:00\n",
       "19      2016-01-02 16:30:37+00:00\n",
       "20      2016-01-02 02:15:05+00:00\n",
       "21      2016-01-02 01:36:47+00:00\n",
       "22      2016-01-02 12:27:42+00:00\n",
       "23      2016-01-02 10:48:44+00:00\n",
       "24      2016-01-02 15:00:26+00:00\n",
       "25      2016-01-02 14:02:16+00:00\n",
       "26      2016-01-02 04:40:15+00:00\n",
       "27      2016-01-02 04:37:15+00:00\n",
       "28      2016-01-02 12:28:59+00:00\n",
       "29      2016-01-02 18:01:17+00:00\n",
       "                   ...           \n",
       "76330   2016-01-02 18:18:39+00:00\n",
       "76331   2016-01-02 02:06:27+00:00\n",
       "76332   2016-01-02 02:07:48+00:00\n",
       "76333   2016-01-02 22:25:46+00:00\n",
       "76334   2016-01-02 22:35:46+00:00\n",
       "76335   2016-01-02 15:46:46+00:00\n",
       "76336   2016-01-02 19:19:26+00:00\n",
       "76337   2016-01-02 01:30:27+00:00\n",
       "76338   2016-01-02 19:14:49+00:00\n",
       "76339   2016-01-02 10:02:25+00:00\n",
       "76340   2016-01-02 07:14:27+00:00\n",
       "76341   2016-01-02 04:59:20+00:00\n",
       "76342   2016-01-02 00:18:28+00:00\n",
       "76343   2016-01-02 07:07:39+00:00\n",
       "76344   2016-01-02 01:12:24+00:00\n",
       "76345   2016-01-02 17:46:49+00:00\n",
       "76346   2016-01-02 20:38:48+00:00\n",
       "76347   2016-01-02 01:59:27+00:00\n",
       "76348   2016-01-02 01:48:27+00:00\n",
       "76349   2016-01-02 01:49:52+00:00\n",
       "76350   2016-01-02 09:26:19+00:00\n",
       "76351   2016-01-02 06:31:37+00:00\n",
       "76352   2016-01-02 05:04:40+00:00\n",
       "76353   2016-01-02 19:47:50+00:00\n",
       "76354   2016-01-02 22:02:57+00:00\n",
       "76355   2016-01-02 02:40:50+00:00\n",
       "76356   2016-01-02 20:31:45+00:00\n",
       "76357   2016-01-02 17:29:51+00:00\n",
       "76358   2016-01-02 23:52:41+00:00\n",
       "76359   2016-01-02 16:55:48+00:00\n",
       "Name: timestamp, Length: 76360, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndat.timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-02\n"
     ]
    }
   ],
   "source": [
    "dirs = sorted(GFW_directories())\n",
    "\n",
    "for i in dirs[1:2]:  # Change!!!\n",
    "\n",
    "    # Get subdirectory list of files\n",
    "    subdir = GFW_DIR + i\n",
    "    allFiles = glob.glob(subdir + \"/*.csv\")\n",
    "    list_ = []\n",
    "    \n",
    "    # Append files in subdir\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_, index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "        dat = pd.concat(list_, axis = 0, ignore_index = True)\n",
    "    \n",
    "    # Append data\n",
    "    outdat = data_step(data=dat)\n",
    "    outdat = outdat.fillna()\n",
    "\n",
    "    # Get string for filename from timestamp\n",
    "    filename = f\"{outdat['year'][1]}-\" + f\"{outdat['month'][1]}\".zfill(2) + f\"-\" + f\"{outdat['day'][1]}\".zfill(2)\n",
    "    \n",
    "    # Save unique mmsi for each day\n",
    "    unique_mmsi_data = outdat['mmsi'].unique()\n",
    "    unique_mmsi = pd.DataFrame({'mmsi':unique_mmsi_data})\n",
    "    unique_mmsi.to_feather('~/Data/GFW_point/Patagonia_Shelf/vessel_list/' + filename +  '_vessel_list'  + '.feather')\n",
    "    \n",
    "    # Save data\n",
    "    outdat.to_csv('~/Data/GFW_point/Patagonia_Shelf/csv/' + filename + '.csv', index=False)\n",
    "    outdat.to_feather('~/Data/GFW_point/Patagonia_Shelf/feather/' + filename + '.feather')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dirs[3:23]\n",
    "def processGFW(i):\n",
    "    subdir = GFW_DIR + i\n",
    "    allFiles = glob.glob(subdir + \"/*.csv\")\n",
    "    list_ = []\n",
    "    \n",
    "    # Append files in subdir\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_, index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "        dat = pd.concat(list_, axis = 0, ignore_index = True)\n",
    "    outdat = data_step(data=dat, filename=i)\n",
    "    outdat.to_csv('~/Data/GFW_point/Patagonia_Shelf/csv/' + i + '.csv')\n",
    "    outdat.to_feather('~/Data/GFW_point/Patagonia_Shelf/feather/' + i + '.feather')\n",
    "    #print(i)  \n",
    " \n",
    "num_cores = 20\n",
    "results = Parallel(n_jobs=num_cores)(delayed(processGFW)(i) for i in inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-01-02', '2016-01-03']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs[1:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseDS_env",
   "language": "python",
   "name": "baseds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
