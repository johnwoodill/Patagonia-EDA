{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import os as os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import gc\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spherical_dist_populate(lat_lis, lon_lis, r=3958.75):\n",
    "    lat_mtx = np.array([lat_lis]).T * np.pi / 180\n",
    "    lon_mtx = np.array([lon_lis]).T * np.pi / 180\n",
    "\n",
    "    cos_lat_i = np.cos(lat_mtx)\n",
    "    cos_lat_j = np.cos(lat_mtx)\n",
    "    cos_lat_J = np.repeat(cos_lat_j, len(lat_mtx), axis=1).T\n",
    "\n",
    "    lat_Mtx = np.repeat(lat_mtx, len(lat_mtx), axis=1).T\n",
    "    cos_lat_d = np.cos(lat_mtx - lat_Mtx)\n",
    "\n",
    "    lon_Mtx = np.repeat(lon_mtx, len(lon_mtx), axis=1).T\n",
    "    cos_lon_d = np.cos(lon_mtx - lon_Mtx)\n",
    "\n",
    "    mtx = r * np.arccos(cos_lat_d - cos_lat_i*cos_lat_J*(1 - cos_lon_d))\n",
    "    return mtx\n",
    "\n",
    "def stationary_vessel(data):\n",
    "    min_d_lat = data.sort_values(['lat', 'lon']).lat.iloc[0]\n",
    "    min_d_lon = data.sort_values(['lat', 'lon']).lon.iloc[0]\n",
    "    max_d_lat = data.sort_values(['lat', 'lon']).lat.iloc[-1]\n",
    "    max_d_lon = data.sort_values(['lat', 'lon']).lon.iloc[-1]\n",
    "\n",
    "    dist = spherical_dist_populate([min_d_lat, max_d_lat], [min_d_lon, max_d_lon])\n",
    "    dist = dist[1,0]\n",
    "\n",
    "    if dist >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "Name: group, dtype: int64\n",
      "min(x) =  50\n",
      "max(y) =  110\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "Name: group, dtype: int64\n",
      "min(x) =  50\n",
      "max(y) =  110\n",
      "3    2\n",
      "4    2\n",
      "5    2\n",
      "Name: group, dtype: int64\n",
      "min(x) =  50\n",
      "max(y) =  100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def subx_y(data):\n",
    "    diff = data['y'].max() - data['x'].min()\n",
    "    print(data['group'])\n",
    "    print('min(x) = ', data['x'].min())\n",
    "    print('max(y) = ', data['y'].max())\n",
    "    #print(diff)\n",
    "        \n",
    "    if diff == 60:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "test = pd.DataFrame({'group': [1, 1, 1, 2, 2, 2], 'x': [50, 55, 60, 50, 50, 50], 'y': [100, 105, 110, 100, 100, 100]})\n",
    "#print(test)\n",
    "grouped = test.groupby('group').apply(subx_y)\n",
    "#print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transform() missing 1 required positional argument: 'func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3be859da5d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mmsi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstationary_vessel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: transform() missing 1 required positional argument: 'func'"
     ]
    }
   ],
   "source": [
    "outdat = test.groupby('mmsi').apply(stationary_vessel).transform()\n",
    "print(outdat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFW_DIR = '/data2/GFW_point/'\n",
    "GFW_OUT_DIR_CSV = '/home/server/pi/homes/woodilla/Data/GFW_point/Patagonia_Shelf/csv/'\n",
    "GFW_OUT_DIR_FEATHER = '/home/server/pi/homes/woodilla/Data/GFW_point/Patagonia_Shelf/feather/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "gfw_vessel_dat = pd.read_csv('~/Data/GFW_public/fishing_vessels/fishing_vessels.csv')\n",
    "dat = pd.read_csv('/data2/GFW_point/2016-01-01/messages-2016-01-01-000000000000.csv')\n",
    "#gfw_identities = gfw_identities.rename(index=str, columns={\"ssvid\": \"mmsi\"})\n",
    "#gfw_identities['year'] = pd.DatetimeIndex(gfw_identities['timestamp']).year \n",
    "#gfw_identities['month'] = pd.DatetimeIndex(gfw_identities['timestamp']).month\n",
    "#gfw_identities['day'] = pd.DatetimeIndex(gfw_identities['timestamp']).day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_step(data): \n",
    "    # Patagonia shelf coordinates\n",
    "    #lower left lat: -58\n",
    "    #lower left lon: -77\n",
    "    #upper right lat: -23\n",
    "    #upper right lon: -22\n",
    "    \n",
    "    # (1) Subset all vessels that are in Patagonia Shelf region\n",
    "    # (2) Keep incomplete segments\n",
    "    lon1 = -77\n",
    "    lon2 = -22\n",
    "    lat1 = -58\n",
    "    lat2 = -23\n",
    "    retdat = data[(data['lon'] >= lon1) & (data['lon'] <= lon2) & (data['lat'] >= lat1) & (data['lat'] <= lat2)]\n",
    "\n",
    "    # Get list of all vessels in region\n",
    "    unique_vessels = list(retdat['mmsi'].unique())\n",
    "\n",
    "    # Subset to allow for all segments even if outside of range\n",
    "    retdat = data[data['mmsi'].isin(unique_vessels)]\n",
    "    \n",
    "    # (3) Remove boats from land and at port\n",
    "    retdat = retdat['distance_from_shore_m' > 0]\n",
    "    retdat = retdat['distance_from_port_m' > 0]    \n",
    "    \n",
    "    # (4) Stationary\n",
    "    retdat = retdat.grouby('mmsi').transform(stationary_vessel)\n",
    "    \n",
    "\n",
    "    \n",
    "    # (5) In/Out of EEZ (country)\n",
    "    \n",
    "    # Separate Year, month, day, hour, minute, second\n",
    "    retdat.loc[:, 'timestamp'] = pd.to_datetime(retdat['timestamp'], format=\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "    retdat.loc[:, 'year'] = pd.DatetimeIndex(retdat['timestamp']).year \n",
    "    retdat.loc[:, 'month'] = pd.DatetimeIndex(retdat['timestamp']).month\n",
    "    retdat.loc[:, 'day'] = pd.DatetimeIndex(retdat['timestamp']).day\n",
    "    retdat.loc[:, 'hour'] = pd.DatetimeIndex(retdat['timestamp']).hour\n",
    "    retdat.loc[:, 'minute'] = pd.DatetimeIndex(retdat['timestamp']).minute\n",
    "    retdat.loc[:, 'second'] = pd.DatetimeIndex(retdat['timestamp']).second\n",
    "    \n",
    "    # Merge GFW ID data\n",
    "    retdat = pd.merge(retdat, gfw_vessel_dat, how='left', on='mmsi')  \n",
    "    \n",
    "    retdat = retdat[['timestamp', 'year', 'month', 'day', 'hour', 'minute', 'second', 'mmsi', 'lat', 'lon', \\\n",
    "                    'segment_id', 'message_id', 'type', 'speed', 'course', 'heading', 'shipname', 'callsign', \\\n",
    "                     'destination', 'elevation_m', 'distance_from_shore_m', 'distance_from_port_m', 'nnet_score', \\\n",
    "                     'logistic_score', 'flag', 'geartype', 'length', 'tonnage', 'engine_power', 'active_2012', \\\n",
    "                     'active_2013', 'active_2014', 'active_2015', 'active_2016']]\n",
    "    \n",
    "    return retdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GFW_directories(directory):\n",
    "    \n",
    "    dirs = os.listdir(directory)\n",
    "    # Remove subfolders 'BK' and 'identities'\n",
    "    if 'BK' in dirs:\n",
    "        dirs.remove('BK')\n",
    "    \n",
    "    if 'identities' in dirs:\n",
    "        dirs.remove('identities')\n",
    "    \n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/home/server/pi/homes/woodilla/.conda/envs/baseDS_env/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n"
     ]
    }
   ],
   "source": [
    "ndat = data_step(data=dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1096"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = sorted(GFW_directories(GFW_DIR))\n",
    "len(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'allFiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-afe9d0cfa616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mallFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'allFiles' is not defined"
     ]
    }
   ],
   "source": [
    "allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GFW_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b49516f712b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGFW_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGFW_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Change!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GFW_dir' is not defined"
     ]
    }
   ],
   "source": [
    "dirs = sorted(GFW_directories(GFW_dir))\n",
    "\n",
    "for i in dirs[1:2]:  # Change!!!\n",
    "\n",
    "    # Get subdirectory list of files\n",
    "    subdir = GFW_DIR + i\n",
    "    allFiles = glob.glob(subdir + \"/*.csv\")\n",
    "    list_ = []\n",
    "    \n",
    "    # Append files in subdir\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_, index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "        dat = pd.concat(list_, axis = 0, ignore_index = True)\n",
    "    \n",
    "    # Append data\n",
    "    outdat = data_step(data=dat)\n",
    "    outdat = outdat.fillna()\n",
    "\n",
    "    # Get string for filename from timestamp\n",
    "    filename = f\"{outdat['year'][1]}-\" + f\"{outdat['month'][1]}\".zfill(2) + f\"-\" + f\"{outdat['day'][1]}\".zfill(2)\n",
    "    \n",
    "    # Save unique mmsi for each day\n",
    "    unique_mmsi_data = outdat['mmsi'].unique()\n",
    "    unique_mmsi = pd.DataFrame({'mmsi':unique_mmsi_data})\n",
    "    unique_mmsi.to_feather('~/Data/GFW_point/Patagonia_Shelf/vessel_list/' + filename +  '_vessel_list'  + '.feather')\n",
    "    \n",
    "    # Save data\n",
    "    outdat.to_csv('~/Data/GFW_point/Patagonia_Shelf/csv/' + filename + '.csv', index=False)\n",
    "    outdat.to_feather('~/Data/GFW_point/Patagonia_Shelf/feather/' + filename + '.feather')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-01-01']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfw_list_dirs = sorted(GFW_directories(GFW_DIR))\n",
    "\n",
    "# Check for missing files\n",
    "# Get csv files from output\n",
    "csv_files = glob.glob(GFW_OUT_DIR_CSV + \"*.csv\")\n",
    "csv_files = [item.replace('/home/server/pi/homes/woodilla/Data/GFW_point/Patagonia_Shelf/csv/', '') for item in csv_files]\n",
    "csv_files = [item.replace('.csv', '') for item in csv_files]\n",
    "\n",
    "feather_files = glob.glob(GFW_OUT_DIR_CSV + \"*.csv\")\n",
    "feather_files = [item.replace('/home/server/pi/homes/woodilla/Data/GFW_point/Patagonia_Shelf/csv/', '') for item in feather_files]\n",
    "feather_files = [item.replace('.csv', '') for item in feather_files]\n",
    "feather_files\n",
    "\n",
    "csv_feather_diff = list(set(csv_files).difference(feather_files))\n",
    "gfw_diff = list(set(gfw_list_dirs).difference(feather_files))\n",
    "gfw_diff.extend(csv_feather_diff)\n",
    "gfw_list_dirs = gfw_diff\n",
    "gfw_list_dirs\n",
    "\n",
    "new_gfw_list_dirs = []\n",
    "for i in gfw_list_dirs:\n",
    "    indate = i\n",
    "    outdate = datetime.strptime(indate, \"%Y-%m-%d\")\n",
    "    outdate = outdate + timedelta(days=-1)\n",
    "    outdate = datetime.strftime(outdate, \"%Y-%m-%d\")\n",
    "    new_gfw_list_dirs.append(outdate)\n",
    "#new_gfw_list_dirs\n",
    "#gfw_list_dirs\n",
    "sorted(gfw_list_dirs)\n",
    "#from datetime import datetime, timedelta\n",
    "#s = gfw_list_dirs\n",
    "#date = datetime.strptime(s, \"%Y-%m-%d\")\n",
    "##modified_date = date + timedelta(days=-1)\n",
    "#modified_date = datetime.strftime(modified_date, \"%Y/%m/%d\")\n",
    "#modified_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dirs\n",
    "def processGFW(i):\n",
    "    subdir = GFW_DIR + i\n",
    "    allFiles = glob.glob(subdir + \"/*.csv\")\n",
    "    list_ = []\n",
    "    \n",
    "    # Append files in subdir\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_, index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "        dat = pd.concat(list_, axis = 0, ignore_index = True)\n",
    "    outdat = data_step(data=dat, filename=i)\n",
    "    outdat.to_csv('~/Data/GFW_point/Patagonia_Shelf/csv/' + i + '.csv')\n",
    "    outdat.to_feather('~/Data/GFW_point/Patagonia_Shelf/feather/' + i + '.feather')\n",
    "    #print(i)  \n",
    " \n",
    "num_cores = 20\n",
    "results = Parallel(n_jobs=num_cores)(delayed(processGFW)(i) for i in inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseDS_env",
   "language": "python",
   "name": "baseds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
